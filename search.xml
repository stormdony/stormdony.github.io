<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NexT主题添加音乐]]></title>
    <url>%2Farchives%2Fbda67445.html</url>
    <content type="text"><![CDATA[NexT主题添加音乐在Hexo博客中添加音乐，有三种方式可以插入音乐 先来预览一下效果： [00:00.780]멈춘 시간 속 [00:03.220]잠든 너를 찾아가 [00:05.430]아무리 막아도 [00:07.440]결국 너의 곁인 걸 [00:09.970]길고 긴 여행을 끝내 [00:12.890]이젠 돌아가 [00:15.060]너라는 집으로 [00:17.310]지금 다시 [00:18.310]way back home [00:40.140]아무리 힘껏 닫아도 [00:42.510]다시 열린 서랍 같아 [00:44.840]하늘로 높이 날린 넌 [00:47.340]자꾸 내게 되돌아와 [00:49.700]힘들게 삼킨 이별도 [00:52.210]다 그대로인 걸 [00:54.260]oh oh oh [00:57.710]수없이 떠난 길 위에서 [01:00.550]난 너를 발견하고 [01:02.810]비우려 했던 맘은 또 [01:05.350]이렇게 너로 차올라 [01:07.850]발걸음의 끝에 [01:10.250]늘 니가 부딪혀 [01:12.550]그만 [01:14.940]그만 [01:17.560]멈춘 시간 속 [01:19.500]잠든 너를 찾아가 [01:22.230]아무리 막아도 [01:24.210]결국 너의 곁인 걸 [01:26.660]길고 긴 여행을 끝내 [01:29.690]이젠 돌아가 [01:31.870]너라는 집으로 [01:33.740]지금 다시 [01:34.900]way back home [01:55.250]조용히 잠든 방을 열어 [01:58.070]기억을 꺼내 들어 [02:00.410]부서진 시간 위에서 [02:02.680]선명히 너는 떠올라 [02:05.300]길 잃은 맘 속에 [02:07.710]널 가둔 채 살아 [02:10.100]그만 [02:12.530]그만 [02:15.200]멈춘 시간 속 [02:17.260]잠든 너를 찾아가 [02:19.770]아무리 막아도 [02:21.820]결국 너의 곁인 걸 [02:24.220]길고 긴 여행을 끝내 [02:27.220]이젠 돌아가 [02:29.330]너라는 집으로 [02:31.260]지금 다시 [02:32.480]way back home [02:34.750]세상을 뒤집어 [02:37.090]찾으려 해 [02:39.660]오직 너로 완결된 [02:41.940]이야기를 [02:45.010]모든 걸 잃어도 [02:49.580]난 너 하나면 돼 [03:02.720]빛이 다 꺼진 여기 [03:05.610]나를 안아줘 [03:13.150]눈을 감으면 [03:14.520]소리 없이 밀려와 [03:17.440]이 마음 그 위로 [03:19.390]넌 또 한 겹 쌓여가 [03:21.880]내겐 그 누구도 아닌 [03:24.740]니가 필요해 [03:27.070]돌아와 내 곁에 [03:29.340]그날까지 [03:30.380]I’m not done var ap = new APlayer({ element: document.getElementById("aplayer-sQXvExXe"), narrow: false, autoplay: false, showlrc: 2, music: { title: "Way Back Home", author: "SHAUN", url: "//music.163.com/song/media/outer/url?id=863046037.mp3", pic: "https://p1.music.126.net/MAkLvm2p9LE0mWLEr2NkMA==/109951163378634466.jpg?param=130y130", } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 一、使用html标签写法如下： 1&lt;audio src="https://什么什么什么.mp3" style="max-height :100%; max-width: 100%; display: block; margin-left: auto; margin-right: auto;" controls="controls" loop="loop" preload="meta"&gt;Your browser does not support the audio tag.&lt;/audio&gt; 二、使用网易云外链网易云音乐的外链很好用，不仅有可以单曲，还能有歌单，有兴趣的自己去网易云音乐找首歌尝试。但是有一些音乐因为版权原因放不了，还有就是不完全支持 https，导致小绿锁不见了。 网易云歌曲外链接获取方法首先 找到你要下载的歌曲 用网页版打开 复制链接中的歌曲ID 如：SHAUN - Way Back Home1https://music.163.com/#/song?id=863046037 ID就是863046037然后将ID替换到下面的链接中1http://music.163.com/song/media/outer/url?id= .mp3 如：1http://music.163.com/song/media/outer/url?id=863046037.mp3 三、安装插件安装插件可以完美的解决上面的问题，并且用插件，有显示歌词功能，也美观，建议使用这种方法。 安装插件首先在站点文件夹根目录安装插件： 所在目录：~/blog/1npm install hexo-tag-aplayer --save 使用方法一在文章中的写法：1&#123;% aplayer title author url [picture_url, narrow, autoplay, width:xxx, lrc:xxx] %&#125; 标签参数 title : 曲目标题 author: 曲目作者 url: 音乐文件 URL 地址 picture_url: (可选) 音乐对应的图片地址 narrow: （可选）播放器袖珍风格 autoplay: (可选) 自动播放，移动端浏览器暂时不支持此功能 width:xxx:(可选) 播放器宽度(默认: 100%) lrc:xxx: (可选）歌词文件 URL 地址 实例1&#123;% aplayer "歌曲名" "歌手名" "https://什么什么什么.mp3" "https://封面图.jpg" "lrc:https://歌词.lrc" %&#125; 方法二除了使用标签 lrc 选项来设定歌词，你也可以直接使用 aplayerlrc 标签来直接插入歌词文本在博客中：123&#123;% aplayerlrc "title" "author" "url" "封面(选填)" "autoplay" %&#125;[00:00.00]lrc here&#123;% endaplayerlrc %&#125; 更多详细使用方法参考文档：hexo-tag-aplayer 获取歌词歌词的获取，可以直接找到各层次文件，或者可以直接在网易云上通过以下方法获取1http://music.163.com/api/song/media?id=863046037 其中id为网易云歌曲的id，打开链接之后，可以把”lyric”字段的值复制下来，再删除\n就可以直接放到aplayerlrc标签中了，这样就可以有歌词出现]]></content>
      <categories>
        <category>NexT</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[leetcode02]9-回文数]]></title>
    <url>%2Farchives%2F4644f3b9.html</url>
    <content type="text"><![CDATA[题目第9题：回文数 判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序(从右向左)读都是一样的整数。示例 1: 输入: 121输出: true 示例 2: 输入: -121输出: false 解释: 从左向右读, 为 -121 。 从右向左读, 为 121- 。因此它不是一个回文数。 示例 3: 输入: 10输出: false 解释: 从右向左读, 为 01 。因此它不是一个回文数。 进阶:你能不将整数转为字符串来解决这个问题吗？ 思路&amp;实现思路：先取出各个数字变成list，再翻转；然后判断新的list与原来的是否一致123456789101112131415class Solution: def isPalindrome(self, x): """ 先取出各个数字组成list，再翻转 :type x: int :rtype: bool """ li = [] if x &lt; 0: return False else: while x != 0: li.append(x % 10) x = int(x / 10) return list(reversed(li)) == li 官方解释首先，我们应该处理一些临界情况。所有负数都不可能是回文，例如：-123 不是回文，因为 - 不等于 3。所以我们可以对所有负数返回 false。 现在，让我们来考虑如何反转后半部分的数字。 对于数字 1221，如果执行 1221 % 10，我们将得到最后一位数字 1，要得到倒数第二位数字，我们可以先通过除以 10 把最后一位数字从 1221 中移除，1221 / 10 = 122，再求出上一步结果除以10的余数，122 % 10 = 2，就可以得到倒数第二位数字。如果我们把最后一位数字乘以10，再加上倒数第二位数字，1 * 10 + 2 = 12，就得到了我们想要的反转后的数字。 如果继续这个过程，我们将得到更多位数的反转数字。 现在的问题是，我们如何知道反转数字的位数已经达到原始数字位数的一半？ 我们将原始数字除以 10，然后给反转后的数字乘上 10，所以，当原始数字小于反转后的数字时，就意味着我们已经处理了一半位数的数字。 复杂度分析时间复杂度：O(\log_{10}(n))O(log 10 (n))， 对于每次迭代，我们会将输入除以10，因此时间复杂度为 O(\log_{10}(n))O(log10(n))。空间复杂度：O(1)O(1)。 实现根据官方的解释，我试着实现了一下：1234567891011121314def gaunfang(self, x): ''' 用时：400ms :param x: :return: ''' if x &lt; 0: return False temp_x = x y = 0 while temp_x != 0: y = y * 10 + temp_x % 10 temp_x = int(temp_x / 10) return y == x 其他方法1234567891011def reversed_str(self, x): ''' 翻转字符串 用时：446ms :param x: :return: ''' if str(x) == str(x)[::-1]: return True else: return False 这个方法主要是采用了字符串截取的方法，从最后逐位翻转，在判断。 题目难度难度：简单 总结这道题的算法难度不大，实现起来简单，但是没有想到的是，python的实现方法会是如此的简洁。看来，还是的多接触一下优秀的代码，增长一下自己的见识才行！！！]]></content>
      <categories>
        <category>python</category>
        <category>算法</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NexT建站历程]]></title>
    <url>%2Farchives%2F55e73569.html</url>
    <content type="text"><![CDATA[最近一段时间一直在倒腾着自己的个人博客。本来直接部署到Github上就算了，没想着弄域名或者搜索引擎优化啥的。但是前几天，忽而兴起，跑到阿里云上搜了一下域名，看到一个喜欢的域名还不错，并且价格也还算亲民。一时冲动就下了单…. 冲动是魔鬼，要不是买了个域名，或许就不会有这么多的烦心事。真的是“ No zuo , No die ”。折腾了好几天，终于弄好了https和引擎收录。加上了小绿锁，感觉就是不一样；虽然现在在搜索引擎上的权重还没上去，但是相信以后会好起来的。 说到搜索引擎，不得不吐槽一下国内和国外的效率。百度都提交一个星期了，才搜得到。人家谷歌第二天就把大部分文章给索引到了。还有一点最烦人的就是，Github 是禁止百度爬虫的 :( 搞得我还得双线部署。幸好，最后还是依靠国内的 Coding ，才让百度爬虫爬取到我的站点，并收录了进去。 都说前人栽树，后人乘凉。感谢各位博主的博客，才让我少踩了很多坑。以后就可以安心的写博客，不用管这些烦心事了。 为了记录一下自己的艰辛填坑路，也为了让更多想搭建博客的人能够 少踩一点坑。下面将献上一些对我有帮助的博客链接，希望也能帮到正在折腾的你！！ GitHub Pages 绑定个人域名，免 Cloudflare 支持 https 打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化 Hexo提交百度和Google收录站点 hexo的next主题个性化教程:打造炫酷网站 号外号外！解决github+hexo+yilia评论插件的问题！！！ 总的来说这几篇博文，已经涵盖了 NexT 主题的所有配置和优化过程。至于其他主题可以自行找教程，不过还是建议选用多人用的主题，避免钻牛角尖，既浪费时间，又难受，何必呢是吧？ 最后，当然是宣传一波我的站点啦。。。欢迎访问分享 ，反馈意见哈！！ https://www.donlex.cn]]></content>
      <categories>
        <category>随笔</category>
        <category>NexT</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>NexT</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode打卡-1]]></title>
    <url>%2Farchives%2Fb6c00922.html</url>
    <content type="text"><![CDATA[前言从今天开始，将会开启我的 LeetCode 打卡之路。为了能让打卡坚持下去，就给自己设定了用博客来记录打卡过程的目标。算是留点记录吧！希望将来会感谢今天努力刷 LeetCode 的自己 : )~ 题目给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 思路采用逆向思维，用目标的值减去数组中的一个数，看结果是否还在数组里面 实现代码实现使用的是 python 语言 123456789101112131415class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in nums: if target - i in nums and i is not target-i: return [nums.index(i), nums.index(target - i)] nums = [2, 7, 11, 15]target = 9s = Solution()print(s.twoSum(nums, target)) 题目难度难度：简单]]></content>
      <categories>
        <category>python</category>
        <category>算法</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python or Java？Boss直聘告诉你该如何选择]]></title>
    <url>%2Farchives%2Fe7a46e9a.html</url>
    <content type="text"><![CDATA[前言“人生苦短，我用 Python”，Python 的经典 slogan 讲究争分夺秒，并且在 9月的TIOBE榜中拿下第 3 名宝座。 今天就试着在Boss直聘网站上爬取python和java的招聘信息，比较一下两个方向的发展钱景，为本科生的就业方向给一个小小的建议 爬取在招聘网站上直接以”本科生”和”java”或”python”作为筛选条件，以广州为例爬取招聘的大体信息，具体代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from bs4 import BeautifulSoupimport requestsimport pymongoclient = pymongo.MongoClient('localhost', 27017)zhipin = client['zhipin']zhipin_java = zhipin['zhipin_java']zhipin_python = zhipin['zhipin_python']headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',&#125;total_page = 11def get_info(param, data_table): ''' 根据招聘方向(java或python..)爬取信息存进数据库 :param param: 招聘方向 :param data_table: 数据库表明 :return: ''' for i in range(1, total_page): url = 'https://www.zhipin.com/c101280100/d_203-h_101280100/?query=&#123;0&#125;&amp;page=&#123;1&#125;'.format( param, i) web_data = requests.get(url, headers=headers) soup = BeautifulSoup(web_data.content, 'lxml') for item in soup.select('#main &gt; div &gt; div.job-list &gt; ul &gt; li'): # 招聘要求 job_title = item.select('.job-title')[0].text # 岗位 salary = item.select('.red')[0].text # 薪资 person_info = item.select('.info-primary p')[0].text # 应聘要求 # 获取公司信息 company = item.select('.info-company h3 a')[0].text # 公司 company_info = item.select('.info-company p')[0].text # 公司信息 data = &#123; 'job_title': job_title, 'salary': salary, 'person_info': person_info, 'company': company, 'company_info': company_info, &#125; # 插入数据库 data_table.insert(data) print(data) print('*' * 100) print('\n' * 5)if __name__ == '__main__': param_list = ['java', 'python'] table_list = [zhipin_java, zhipin_python] for param, table in zip(param_list, table_list): get_info(param, table) 爬取的信息全部存在mongodb中。便于后面的分析处理 数据清洗在数据处理这里定义了几个方法，用来处理相应的内容 1.初始化变量12345678import pymongoclient = pymongo.MongoClient('localhost', 27017)zhipin = client['zhipin']zhipin_java = zhipin['zhipin_java']zhipin_python = zhipin['zhipin_python']from collections import Counterfrom pyecharts import Bar,Line,Pie 2.获取地区分布情况123456789101112131415import redef get_zone(): ''' 获取地区''' zone_list = [] real_list = [] for item in zhipin_java.find(): text = item['person_info'][3:6] zone_list.append(text) for i in zone_list: j = re.sub(r' \d-','',i) real_list.append(j) while '' in real_list: real_list.remove('') return real_listzone = dict(Counter(get_zone())) 3.整理招聘数据123456789def del_key_1(): '''删除招聘次数为1的岗位''' li = [] for key in job_dict.keys(): if job_dict[key] == 1: li.append(key) for i in li: del job_dict[i] print(job_dict) 4.整理薪水数据1234567891011def get_salary(): '''获取招聘的工资''' min_list = [] #起步工资 max_list = [] #最高工资 job_title = [] #岗位 for item in zhipin_java.find(): job_title.append(item['job_title']) salary = item['salary'] min_list.append(int(salary.split('-')[0][:-1])) max_list.append(int(salary.split('-')[1][:-1])) return min_list,max_list,job_title 数据可视化地区分布通过整理地区分布数据,利用pyecharts作图 1234bar = Bar("java和python岗位地区分布")bar.add("java", list(key for key in zone.keys()), list(value for value in zone.values()),mark_line=['min', 'max'], is_toolbox_show = True,is_more_utils=True)bar.add("python", list(key for key in py_zone.keys()), list(value for value in py_zone.values()),mark_line=['min', 'max'], is_toolbox_show = True,is_more_utils=True)bar 越靠近城市中心的地区，招聘的岗位就越多，成功应聘的机会较高；番禺和天河区相差较大，其中天河区招python比java将近多8倍；番禺区java比python更加热门，受公司青睐；其他区相差不大 招聘最多的岗位python岗位： 占比前五位分别是： python工程师 数据分析师 运维工程师 大数据开发工程师 游戏AI算法工程师 java岗位对比 高级的工程师招聘的人数较少，大部分都是在招聘初中级工程师，难道这就是传说中的“一个诸葛亮胜过三个臭皮匠 (:” 公司对比python招聘公司 java招聘公司 最关心的钱途问题最高薪水 看来python不是吹的，最高薪水也大多数都比java的高;java最高薪水平均19.24K，最低3K，最高50k；python最高薪水平均21.16K,最低3k，最高60k 最低薪水 python起步薪水大多数都比java的高;java平均起步薪水11.42K，python平均起步薪水12.08K 两个岗位词云 源码：https://github.com/stormdony/python_demo CSDN博客：https://blog.csdn.net/stormdony/article/details/82586735 1ps: 原创文章，转载请与作者联系]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo下yilia主题添加字数统计和阅读时长功能]]></title>
    <url>%2Farchives%2Fe9dc5279.html</url>
    <content type="text"><![CDATA[1.安装 hexo-wordcount在博客目录下打开Git Bash Here 输入命令 npm i –save hexo-wordcount 2.文件配置在theme\yilia\layout\_partial\post下创建word.ejs文件： 123456789101112131415161718&lt;div style="margin-top:10px;"&gt; &lt;span class="post-time"&gt; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-keyboard-o"&gt;&lt;/i&gt; &lt;span class="post-meta-item-text"&gt; 字数统计: &lt;/span&gt; &lt;span class="post-count"&gt;&lt;%= wordcount(post.content) %&gt;字&lt;/span&gt; &lt;/span&gt; &lt;/span&gt; &lt;span class="post-time"&gt; &amp;nbsp; | &amp;nbsp; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-hourglass-half"&gt;&lt;/i&gt; &lt;span class="post-meta-item-text"&gt; 阅读时长: &lt;/span&gt; &lt;span class="post-count"&gt;&lt;%= min2read(post.content) %&gt;分&lt;/span&gt; &lt;/span&gt; &lt;/span&gt;&lt;/div&gt; 然后在 themes/yilia/layout/_partial/article.ejs中添加123456789101112131415&lt;div class="article-inner"&gt; &lt;% if (post.link || post.title)&#123; %&gt; &lt;header class="article-header"&gt; &lt;%- partial('post/title', &#123;class_name: 'article-title'&#125;) %&gt; &lt;% if (!post.noDate)&#123; %&gt; &lt;%- partial('post/date', &#123;class_name: 'archive-article-date', date_format: null&#125;) %&gt; &lt;!-- 需要添加的位置 --&gt; &lt;!-- 开始添加字数统计--&gt; &lt;% if(theme.word_count &amp;&amp; !post.no_word_count)&#123;%&gt; &lt;%- partial('post/word') %&gt; &lt;% &#125; %&gt; &lt;!-- 添加完成 --&gt; &lt;% &#125; %&gt; &lt;/header&gt; 3. 开启功能在站点的_config.yml中添加下面代码123# 是否开启字数统计#不需要使用，直接设置值为false，或注释掉word_count: True ps：原创文章，转载请注明出处]]></content>
      <categories>
        <category>yilia</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>yilia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加和取消live2d看板动画]]></title>
    <url>%2Farchives%2F14d1161.html</url>
    <content type="text"><![CDATA[添加看板娘在博客目录下安装依赖 npm install –save hexo-helper-live2d 在主题下的_config.yml的配置信息Hexo的 _config.yml 文件添加配置. 示例:12345678910111213141516live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: use: live2d-widget-model-wanko display: position: right width: 150 height: 300 mobile: show: false 使用其他的模型，需要先安装模型,在修改配置信息中的use 查看模型：https://github.com/xiazeyu/live2d-widget-models 截图预览：https://huaji8.top/post/live2d-plugin-2.0/ 安装模型 npm install 模型的包名 具体可以查看官方文档：https://github.com/EYHN/hexo-helper-live2d/blob/master/README.zh-CN.md 取消看板娘直接运行下面的命令 npm uninstall hexo-helper-live2d 去掉站点_config.yml下的配置信息即可 ps：原创文章，转载请注明出处]]></content>
      <categories>
        <category>yilia</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>yilia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单选题：安全和便捷，你选哪一个？]]></title>
    <url>%2Farchives%2F4ec71def.html</url>
    <content type="text"><![CDATA[前段时间的滴滴杀人案可谓是闹得满城风雨，但还是禁不住时间这把杀猪刀，人们的眼球又被“东哥”的案子给吸引过去了。虽然吃瓜看戏不错，但是也要关注一下关乎自身利益的问题。9月4日晚，滴滴出行宣布将于2018年9月4日启动安全大整治，2018年9月8日23点至9月15日凌晨5点期间在中国大陆地区暂停提供深夜23：00-5：00时间段的网约车。 这一措施一旦实行，将会在出行和安全方面带来怎样的变化？ 首先很多加班族和夜猫子可能会心慌的一批。深夜刚加完班，公交、地铁都停了，只能在路边使劲摇胳膊喊“师傅”，本来上班的压力就山大了，现在可能还要体验一把抢车风波；而蹦迪的、买醉的还有撸串的出行难度也会增加，最后可能会放弃，选择宅起来。 1.出行成本相对于出租,虽然滴滴价格并没有低太多,但还是便宜一些。并且网约车的接单机制与出租车或者其他的出行方式的不一样。网约车可以自己设定时间，更加迁就自己，不必浪费时间在等车上；而其他的方式则有一定的时间随机性。在时间成本上，网约车的消耗成本就低了很多。 另外在网约车的市场上，滴滴占了63%的市场份额。而停止深夜服务则会更加加剧出行选择的稀缺性。“物以稀为贵”的道理大家都明白。这样可能不仅是时间和金钱成本上的的增加了。 2.安全 | 便捷滴滴的几次用户遇害案件，使得乘客的警惕意识加强，同时也让社会对网约车的有了防备。很多人虽然也对先前发生的事情义愤填膺，但是经受不了站街摇手苦等，于是会想，滴滴用户量那么大，遭遇不幸的事情总不会落到我头上吧，并且滴滴还上线了“一键报警”功能… 虽然想象很美好，但是在整改的成效没有体现出来之前，安全和便捷不能同时满足。而滴滴的做法意思就很明显了。我们也只能牺牲便捷来换取安全保障的提高了。 3.用户心理网约车跟私人订制类似，而出租和其他的出行方式是面向大众的。这一区别使得用户的体验感不一样，一个是消费者是上帝（真实是不是这样就另说了），另一个则体现不出个人的与众不同。 最后试问一句：如果没有安全方面的问题，你愿意在路边苦等出租，还是使用滴滴这样的定制服务？？ ps：纯属个人看法，有不成熟之处请谅解！！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫利器-cURL转换]]></title>
    <url>%2Farchives%2Ffd7c0b75.html</url>
    <content type="text"><![CDATA[前言在爬虫的过程，经常需要为程序添加请求头，参数，cookie等信息，但是这些信息的添加都需要手动的去浏览器中找，然后一点一点的慢慢复制粘贴，这样效率就非常的低了。今天就分享一个网站，解决这些问题，让你脱离这些没有意义的劳动 网站介绍网址: https://curl.trillworks.com 从上图可以看到网站的教程，只要根据教程三步走，就可以快速的添加相应的请求信息 示范 将需要爬取的请求复制curl到网站中转换，然后复制到pycharm中就可以直接爬取到整个网站的源码了，接下来就可以直接在这个基础上开始逻辑工作了 生成的代码：123456789101112131415161718192021222324import requestscookies = &#123; '_octo': 'GH1.1.681056136.1509806877', '_gat': '1', 'logged_in': 'no', '_ga': 'GA1.2.70269906.1509806877', 'tz': 'Asia%2FShanghai', 'has_recent_activity': '1', '_gh_sess': 'cGpmdExmZUZpckZ0R1pSQlFxZlpsS2ZvT3NZbUU0YW1qTVloSzdFeWNxeWdNaGxsNzVveTJ3Vndrc2ZaN3ZoRDNYMm10TW9OdUdGVHhwbVRmMEU3ZWVwTUx4dUpZTUgrbHdKZkV0RnpzN3hodG12TGdLbHpSemVaQ0ZMM201MGdxMlkxdk5JNUZ6em1SWGp5ZEJUYTNQMjRFcCtqUDZaWVVFNXl3VDJRRUU4MFpqYkpvekY1VmZpY2t1R01ZcGRPQlZBUEJUOTJaWnNESjVnMnlkcncyWWhCVDl1OE5aVDhpR2Z4Z1NYVkFVNk5ReDRtTVphOXFXQWJNSVZYcnEyVktLTERLMHBTYjNwa2tUQUJaaWREQ0N4NzJYTG9sM1dpUktPaWFETFVpWGZlWFNvb2ZxazU1OUxMazVjZ3VNNTJteEdENzJPQlFKeDV3YXZCbmdHSGdGVmx5OVNjU2VaZXh3eEVwSlptczZXV3lQZXgrOGEyVGFwcUpPcFhIZTRWaDIwZExMRWhDRE8yMUdJT2xmS1grQ3I3bEYySGJvWFhNTFR3VmNpRnlLTT0tLXlRMmJZanl4Z0tUU0c0N1ZrRHpqbkE9PQ%3D%3D--1899440138004359a97b156d0ac8941135684ab5',&#125;headers = &#123; 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer': 'https://ghbtns.com/github-btn.html?user=NickCarneiro&amp;repo=curlconverter&amp;type=watch&amp;count=true&amp;size=large', 'Connection': 'keep-alive', 'Cache-Control': 'max-age=0',&#125;response = requests.get('https://github.com/NickCarneiro/curlconverter/', headers=headers, cookies=cookies) 可以看到生成的代码非常的规整，是不是很方便~~]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Farchives%2F4a17b156.html</url>
    <content type="text"><![CDATA[Welcome to DonLex’s blog. This blog has just been established, so here is few pages, it will be improved gradually.If you have any problems, you can send E-mail to donlex@qq.com]]></content>
  </entry>
</search>
