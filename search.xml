<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python包的管理利器pipenv]]></title>
    <url>%2Farchives%2F18ef6217.html</url>
    <content type="text"><![CDATA[大家都知道 pip（pip2、pip3分别对应python的2、3版本）是python的包管理工具，使得python的第三方包的安装等任务十分方便。另外，很多python的虚拟环境工具如virtualenv、pyenv等等都得到了广泛使用。 而pipenv是requests库作者Kenneth Reitz开发的，它结合了pip及virtualenv的功能和优点，其目的是替代virtualenv和pyenv，将pip及virtualenv的功能集于一身。 多说一句，尽管 pip 可以安装 Python 包，但仍推荐使用 pipenv，因为它是一种更高级的工具，可简化依赖关系管理的常见使用情况。如果还没有使用过pipenv的童鞋，赶紧试试吧！ pipenv安装pipenv 的安装依赖 pip，如果没有配置好pip，安装pipenv还是有点麻烦的。如果配置好了pip的环境变量，在任意目录下都可以使用下面的命令安装 pip install pipenv pipenv使用方法 命令名 命令 创建pipenv pipenv 启动pipenv pipenv shell(若未存在虚拟环境会自动创建) 退出pipenv exit 查找所有安装包 pip list 环境内包的依赖展示 pipenv graph 查找虚拟环境的路径 pipenv –venv 卸载安装包 pipenv uninstall 实例将目录更改为包含你的Python项目的文件夹，并启动Pipenv， cd my_projectpipenv install 这将在项目目录中创建两个新文件: Pipfile和Pipfile.lock，如果项目不存在，则为项目创建一个新的虚拟环境。Pipfile包含关于项目的依赖包的信息，并取代通常在Python项目中使用的requirements.txt文件。 在上图中，使用了mkdir创建了文件夹PyProject，之后进入这个文件夹使用pipenv install 创建了虚拟环境。由于默认的python版本是3的，所以创建的虚拟环境是py3。如果需要使用py2的话，可以添加参数进行创建。 pipenv install - -two 激活虚拟环境： pipenv shell 激活后，可以看到控制台输入那里已经标明虚拟环境了 ###安装第三方库 使用pipenv创建虚拟环境后，进入pipfile所在目录，使用install命令安装第三方库。 例如 pipenv install requests 其实把pipenv当作pip来使用。无需像virtualenv那样需要额外的先启动虚拟环境。pipenv区分你是在给哪个虚拟环境安装，依赖的是Pipfile文件的位置。 卸载第三方库 pipenv uninstall requests 查看项目中安装的依赖 pipenv graph 冻结Pipfile冻结就相当于将项目所使用的第三方库列表进行打包输出，类似于的virtualenv中生成requirements.txt文件。 通过更新Pipfile.lock来冻结库名称及其版本，以及其依赖关系的列表需要使用锁参数： pipenv lock 如果另一个用户拷贝了你的项目，他们只需要安装Pipenv，然后： pipenv install pipenv会在项目文件夹下自动寻找Pipfile和Pipfile.lock文件，创建一个新的虚拟环境并安装必要的软件包。 最后如果你在做web项目就会体会到pipenv的便利。一些包只支持py2，所以必须使用Python2，而现在都8102年了，大部分人都习惯py3了，所以用pipenv创建虚拟环境能够避免混乱，而且pipenv也比其他的一些管理工具更加人性化一点。 更多详细指令可以直接输入pipenv查看，或者查看官方文档。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的2018总结]]></title>
    <url>%2Farchives%2F58a24148.html</url>
    <content type="text"><![CDATA[转眼间，这趟列车就要驶过2018这个站点了。在这一年里，我个人成长了很多，也收获了很多。感谢一直以来关注我的读者，是你们的支持，让我坚持到了现在。 其实我个人开始有意识的积累，应该是在2018年3月份之后。因为3月份那个时候，刚好受到多个公众号文章的影响，逐渐地开通了博客，之后不久才开通了公众号“Python绿洲”。 Python绿洲 下面就分享一下我个人在2018年的一些经历。 3月份前的我：盲目生长在这段时间之前，我只是盲目的跟随学校的脚步，学着学校安排的课程，没有多么地刻苦勤奋，所以也没有取得多么好的成绩，感觉基本上都是在玩手机中度过了大学中最初的两年。可能在学校里，没有外面那么大的压力，对于就业也没有很好的急迫感，只是随波逐流而已。如果一直都过着学校的平静生活，没有意外的话，我应该就会成为毕业即失业的正面教材。 在3月份，大二下学期生活刚开始的时候，我逐渐有了一些想法：不想再这样浑浑噩噩下去了，我要改变现在这种颓废的状态。想要改变就要走出舒适区，让学校生活这一潭死水搅动起来。 也得益于大二的时候有学过一点Python的基本知识，让我找到了方向：在不放弃学校的Java方向的前提下，努力朝Python方向深入。 3月份后的我：野蛮而向上初识爬虫既然确定了方向，就要不断朝着这个方向努力。那个时候对Python的能干什么还很朦胧，没有什么想法，觉得无从下手。在跟一个比较聊得来的师兄的建议下，选择了从爬虫学起。之后就通过看视频学习爬虫，不断地在网上找案例来练手。在学习的这段时间里，我基本都是在晚上2、3点之后才舍得去睡觉。在看视频–&gt;做案例–&gt;写博客的循环中，我能够很清楚地看到我的学习成果，每一天都让我充满了成就感，不断地鼓励我前进。或许这就是爬虫的魅力吧！在将近一个月的学习下，让我的基本功得到了一定的增强。同时，这一个月也是我的博客最高产的时候。 博客归档 投稿文章第一次投稿文章是在4月末，《后来的我们》刚上映后我就把豆瓣上的影评用selenium给爬了下来并写在了CSDN博客上。之后就发邮件投给菜鸟学python。邮件发出去几天都没有收到回复，本来以为没有希望的时候，收到了回复。非常的激动，也正是投稿的成功，让我坚定了继续学下去的决心。 其实说来有点惭愧，第一次投稿我是奔着稿费去的。作为农村的孩子，我是希望能够尽量少花父母的钱。虽然这次稿费不多，但这是依靠我自己学习的知识赚来的。 更新公众号在第一次投稿的文章：《 爬取豆瓣短评，刘若英导演的电影&lt;后来的我们&gt;发现爱情原来是这样》发布之后，我开始慢慢地将博客中的文章搬过来公众号，并且写一些文章发在公众号。 前期文章的内容杂乱、排版都不够美观。后来在慢慢的接触、模仿一些优秀的公众号文章下，逐步改进了这些缺点。公众号的文章都是在学习之余抽空写出来的，有时候一篇文章就花了好几个小时，所以更新的频率不高，但现在也慢慢地积累了70多篇文章。 在有比较好的想法的时候，努力将这些想法实现，然后写成文章记录下来，并向比较知名的公众号投稿。其实现在大部分的关注者应该都是通过其他公众号流过来的。 由于没有增长黑客的概念和本身不够优秀的原因，导致公众号的关注人数一直不温不火。 不断学习在学习爬虫两个月也就是7月份之后，因为受够了爬取页面的结构不断地改变和反爬措施的不断升级，对爬虫产生了厌烦。我开始转向web方向，学习了Django和Flask，同时还在了解了一些数据分析与机器学习的内容，参加了科赛的机器学习训练营。在不断深入学习的过程中，我体会到了一个本科生知识局限性，而且真的是少的可怜，不得不放慢脚步，回头恶补高数、线代等知识。一直到现在，我都还在慢慢的学习的路上，虽然进展缓慢，但是还是希望能够坚持下去吧！ 自建博客站点在9月份左右，萌生了自己搭建博客的想法。于是，不断实践，学会了使用hexo搭建博客，学会了域名的分配与配置，学会了博客主题的个性化设置。在搭建的过程中，遇到了很多困难，也通过不断地搜索、实践，逐步养成了解决问题的正确习惯：关键字百度谷歌。相信这些路上的绊脚石，会成为我的垫脚石，不断提高自身实力。 域名购买 现在 https://donlex.cn 这个站点建站已经有3个月了，文章还没有发多少。 展望2019其实，我对于未来现在还没有什么规划，真的，可能是还没有踏入社会的原因吧。但是现在大三了，心里也总是会有一些焦虑的。我现在只希望在即将到来的19年里，能够找到自己的方向，并踏踏实实地提升自己的认知水平和专业能力吧！ 至于博客和公众号，我会继续坚持下去。学习之余，我会尽可分享更多对大家有帮助的文章。在这里我不敢做什么保证。另外多说一句，经营一个公众号真的很不容易，写原创文章更是不容易。 就到这里吧，希望大家2019都能够更进一个台阶！感谢一直以来支持我的朋友，薛薛你们！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过猎聘的招聘信息找出学习线路规划]]></title>
    <url>%2Farchives%2F8a901d69.html</url>
    <content type="text"><![CDATA[1文章首发于微信公众号：Python绿洲 。如需转载，请联系该公众号 前言之前发过一篇文章《 Python or Java？Boss直聘告诉你该如何选择》, 主要是这两种语言方向在应聘岗位上的比较，没有涉及具体的岗位要求的分析。前两天，刚好看到网上的一些学习路线图，又对比了招聘网站上的要求，发现这两者其实差不多。所以就用爬虫爬取了猎聘网上的岗位信息，对这些招聘信息进行处理，从中找出需要掌握的一些语言和工具，从而有目的地进行学习，更快的提高自己的能力，让自己不再因为不知道学习的线路而烦恼，也能够符合招聘的要求。 查看网页 (搜索首页) (岗位要求)从这个岗位职责中，可以看出需要掌握的工具或语言绝大多数都是以英文为主。所以主要提取英文就行了。至于其他的信息就不进行提取了 流程 爬取搜索到的岗位链接 进一步爬取详细的岗位要求 提取其中英文单词 pyecharts展示 爬取招聘岗位使用beautifulsoup进行解析,主要将岗位名称，详细链接，公司，薪资，位置和学历要求这几个信息存到MongoDB中就好。 1234567891011121314151617181920212223def getLink(seachname, pagenum): for i in range(pagenum): url = "https://www.liepin.com/zhaopin/?init=-1&amp;key=&#123;&#125;&amp;curPage=&#123;&#125;".format( seachname, i) web_data = requests.get(url=url, headers=header) soup = BeautifulSoup(web_data.content, 'lxml') job_list = soup.select(".sojob-list &gt; li") for item in job_list: name = item.select(".job-info &gt; h3")[0]['title'] link = item.select(".job-info &gt; h3 &gt; a")[0]['href'] company = item.select(".company-name &gt; a")[0].text salary = item.select(".text-warning")[0].text location = item.select(".area")[0].text education = item.select(".edu")[0].text data = &#123; "title": name, "link": link, "company": company, "salary": salary, "location": location, "education": education, &#125; pywork.insert(data) #使用MongoDB存储 详细岗位要求由于任职要求中有&lt;br&gt;标签，需要将其切除，而且由于使用beautifulsoup解析，所以&lt;br&gt;是tag对象，需要创建对象再删除。被这个问题困住了好久。之后将所有爬取到的岗位要求都写到一个文件中，方便后期使用jieba切分 12345678910111213141516171819def getInfo(url, demands_text): web_data = requests.get(url, headers=header) soup = BeautifulSoup(web_data.content, 'lxml') try: demands = soup.select(".content-word")[0].contents demands = sorted(set(demands), key=demands.index) # 删除&lt;br/&gt; delete_str = "&lt;br/&gt;" br_tag = BeautifulSoup(delete_str, "lxml").br demands.remove(br_tag) # 拼接所有要求 for item in demands: demands_text += item.replace("\r", "") #写入文件 f = open('demands.txt', mode='a+', encoding='UTF-8') f.write(demands_text + "\n") f.close() except: logging.log("warning...") 分词使用jieba分词之后，还需要将一些单词例如：or，pc等上删除，本着“宁可错杀一千,不可放过一个”的原则，所以将少于1个字母的单词使用正则去掉 12345678def CutWordByJieBa(txt, seachname): seg_list = jieba.cut(txt, cut_all=True) w1 = "/ ".join(seg_list) # 全模式 fil = re.findall('[a-zA-Z]&#123;1,&#125;/', w1) # 提取英文 strl = "" for i in fil: strl += i strl = strl.lower() # 全部转换为小写 可视化在这部分，之前只想着将需要掌握的工具用词云进行展示就好。然而，有点幸运呀！当使用Navicat12连接到MongoDB的时候，发现它有自动作图分析的功能。所以先用Navicat中的功能，简单的来看一下总体情况： 以爬虫工程师为关键词查询后，使用pyecharts进行数据展示 重点 个人感觉词云更加准确，不过词云和bar图处理方式都是一样的，只是表现形式不一样而已。从柱状图中可以看出需要掌握的工具可以分为三类： 数据库：mysql、oracle、mongodb 解析：xpath、css 反爬：cookie、ip、scrapy、jwt 其中jwt(JSON Web Token) 我就不认识。所以通过这种方式，我就可以找到自己的盲区，就算不深入了解学习，但是百度一下，大概知道它是什么，还是可以的嘛 最后如果有感兴趣的小伙伴，可以自己动手试一下。个人感觉这些排名前10的工具对自己的职业规划还是有点帮助的，也希望能够对你有所帮助！ github地址：https://github.com/stormdony/python_demo/tree/master/LiePin 欢迎关注微信公众号：Python绿洲，获取更多内容！ 文章首发于微信公众号：Python绿洲 。如需转载，请联系该公众号]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel常用函数]]></title>
    <url>%2Farchives%2Fe9dc5278.html</url>
    <content type="text"><![CDATA[​在处理数据的时候，经常使用Python中的Pandas包来处理，有时候即使是很小的数据量，也使用Pandas来处理。个人觉得这有点大材小用，并且有点浪费时间。所以为了能够快速的处理这些小型的数据，最近学习了如何利用Excel来处理。感觉这样比使用Pandas处理得到的结果快速便捷很多。下面将分享几个比较常用的函数。 1.COUNTIF函数countif(range,criteria): 对区域中满足单个指定条件的单元格进行计数 range: 要计算的单元格范围criteria：计算条件，其形式可以为数字、表达式或文本；例如：条件可以为32、“32”，“&gt;32”或“apples” 例子：利用countif函数统计重复值 具体步骤： 选中B2单元格，然后输入函数公式：=COUNTIF(A:A,A2) 将公式复制到B3:C11的所有单元格 2.IF函数if(logical_test,true_value,false_value): 执行真假值判断，根据逻辑计算的真假值，返回结果 logical_test: 表示计算结果为True或False的表达式true_value: 为True时返回的值false_value: 为False时返回的值注意： 条件表达式是用比较运算符(&lt;、=、&gt;)建立的式子，无比较就无判断 两个值若是数值数据可以直接书写，若是文本数据则要使用双引号标记 参数里面所有符号都是英文状态下的标点符号 IF函数可以进行嵌套，但是嵌套层数有限制 3.OR和AND函数OR(logical1,[logical2],…): 至少一个参数为真，就返回TrueAND(logical1,[logical2],…): 所有参数全为真，就返回True logical1：要检验的第一个条件，结果可以为TRUE或FALSElogical2：为可选项，最多可包含255个结果 4.LEFT和RIGHT函数LEFT(text,[num_chars]): 得到字符串左部指定个数的字符RIGHT(text,[num_chars]): 得到字符串右部指定个数的字符 text: 包含要提取的字符的文本字符串num_chars：指定要由LEFT或RIGHT提取的字符的数量 5.CONCATENATE函数concatenate(text1,text2,…) 将几个文本字符串合并为一个文本字符串 text1，text2，…: 需要合并的第1、 2、… 、N个文本项(N&lt;=30),这些文本项可以文本字符串、数字或单个单元格的引用 注意：在将数字和文本合并到一个单元格中时，数字将转换成文本，而不再用做数字，也就是说，无法再对其进行任何数字运算 6.日期函数在一些工作表中，经常需要使用日期，手动输入的话会严重的影响工作效率；使用日期函数就能够快速的实现日期的填写 显示 公式 快捷键 2018-10-29 =TODAY() Ctrl+; 21:33 无 Ctrl+Shift+; 2018-10-29 21:33 =NOW() “Ctrl+;(分号)”,再按空格键，接着按”Ctrl+Shift+;” 7.VLOOKUP函数vlookup匹配函数：在表格的首列查找指定的数据，并返回指定的数据所在行中的指定列处的单元格内容内容公式：VLOOKUP(lookup_value,table_array,col_index_num,range_lookup) lookup_value: 要在表格或区域的第一列中查找的值，其参数可以是值或引用table_array: 包含数据的单元格区域，可以使用绝对区域(如：A2:D8)或区域名称的引用。 table_array第一列中的值是由lookup_value搜索的值。这些值可以是文本、数字或逻辑值col_index_num: 希望返回的匹配的列序号，其参数为1时，返回table_array第一列中的值，以此类推range_lookup: 近似匹配(1)还是精确匹配(0),一般情况选0 例子： 员工职位表 ↑ 员工个人信息 ↑使用VLOOKUP函数将个人的职位填写到信息表中： 打开职位表和个人信息表 在信息表中的F2单元格中输入公式=VLOOKUP(B3,[员工职位表.xlsx]Sheet1!$B$2:$D$11,3,0),按Enter键。注意，输入VLOOKUP函数的第二个参数时，不需要手动输入，直接选中“职位表”中的B1:D11的区域，参数将自动录入成[员工职位表.xlsx]Sheet1!$B$2:$D$11,3,0) 复制F2单元格，并粘贴至F3:F7，即完成数据提取。 结果： 总结在Excel中还有许多有用的函数，由于篇幅的原因就不列举了。这些函数有时候比直接使用Python来处理数据要简单的多。对于数据的处理不管是使用Python也好，Excel也罢，只要能够快速的完成任务，就不失为一种好方法。真的不要拘泥于工具！文章首发于 慕课网]]></content>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web项目中使用支付宝接口]]></title>
    <url>%2Farchives%2Feea56fba.html</url>
    <content type="text"><![CDATA[前言一直想弄一个有关于支付的第三方接口调用的web项目，网上看了一大堆资料，最后还是选择了使用支付宝。原因有一下两点： 不用另外注册账户(一直讨厌注册各种账户) 支付宝有沙箱功能，可以实现虚拟支付，适合拿来练手 下面就进入正题吧 进入沙箱支付宝扫码登录蚂蚁金服开放平台之后，在首页的常用功能那里进入沙箱应用，如果是第一次使用，就需要添加功能。参考下图 生成RSA密钥进入沙箱应用之后可以看到信息配置部分需要设置RSA2(SHA256)密钥，由于我已经设置过了，这里就没有相应的截图。 生成RSA密钥的工具官方也提供了，直接查看生成RSA密钥进入下载工具，并按照步骤就可以生成密钥了。 上传密钥通过上一步生成的密钥，直接在沙箱应用中上传，如果验证通过的话，就可以开始接入代码了。 下载demo在官网中下载demo：https://docs.open.alipay.com/270/106291/ ,将demo解压。然后下载沙箱版的支付宝(只支持安卓版)，然后回到沙箱环境中，查看沙箱账号，用买家信息登录。 合并入自己的项目下面就以一个简单的例子开始，具体步骤如下： 将解压文件中的jar包放入项目lib中,把 jsp文件 或者自己写的页面复制入项目， 修改AlipayConfig文件的部分配置 编写简单的servlet 运行查看支付 详细的目录结构如下： AlipayConfig: 123456789101112131415161718192021222324252627282930public class AlipayConfig &#123; //↓↓↓↓↓↓↓↓↓↓请在这里配置您的基本信息↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ // 应用ID,您的APPID，收款账号既是您的APPID对应支付宝账号 public static String app_id = "填入沙箱APPID"; // 商户私钥，您的PKCS8格式RSA2私钥 public static String merchant_private_key = "填入你的密钥"; // 支付宝公钥,查看地址：https://openhome.alipay.com/platform/keyManage.htm 对应APPID下的支付宝公钥。 public static String alipay_public_key = "填入公钥"; // 服务器异步通知页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数，必须外网可以正常访问 public static String notify_url = "http://localhost:8080/alipay.trade.page.pay-JAVA-UTF-8/notify_url.jsp"; // 页面跳转同步通知页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数，必须外网可以正常访问 public static String return_url = "http://localhost:8080/alipay.trade.page.pay-JAVA-UTF-8/return_url.jsp"; // 签名方式 public static String sign_type = "RSA2"; // 字符编码格式 public static String charset = "utf-8"; // 支付宝网关 public static String gatewayUrl = "https://openapi.alipaydev.com/gateway.do"; // 支付宝网关 public static String log_path = "C:\\"; servlet代码: 这里写了一个简单的doGet方法，当请求这个servlet的时候就会执行支付，然后使用沙箱版的支付宝直接扫码或者网页登录沙箱应用账号就可以支付了 12345678910111213141516171819202122232425262728293031323334353637383940protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //获得初始化的AlipayClient AlipayClient alipayClient = new DefaultAlipayClient(AlipayConfig.gatewayUrl, AlipayConfig.app_id, AlipayConfig.merchant_private_key, "json", AlipayConfig.charset, AlipayConfig.alipay_public_key, AlipayConfig.sign_type); //设置请求参数 AlipayTradePagePayRequest alipayRequest = new AlipayTradePagePayRequest(); alipayRequest.setReturnUrl(AlipayConfig.return_url); alipayRequest.setNotifyUrl(AlipayConfig.notify_url); //商户订单号，商户网站订单系统中唯一订单号，必填 String out_trade_no = "20181037"; //付款金额，必填 String total_amount = "9999"; //订单名称，必填 String subject = "支付宝测试"; //商品描述，可空 String body = "商品描述"; alipayRequest.setBizContent("&#123;\"out_trade_no\":\""+ out_trade_no +"\"," + "\"total_amount\":\""+ total_amount +"\"," + "\"subject\":\""+ subject +"\"," + "\"body\":\""+ body +"\"," + "\"product_code\":\"FAST_INSTANT_TRADE_PAY\"&#125;"); //请求 String result; try &#123; result = alipayClient.pageExecute(alipayRequest).getBody(); response.setContentType("text/html;charset=" + AlipayConfig.charset); response.getWriter().write(result);//直接将完整的表单html输出到页面 response.getWriter().flush(); response.getWriter().close(); &#125; catch (AlipayApiException e) &#123; e.printStackTrace(); response.getWriter().write("捕获异常出错"); response.getWriter().flush(); response.getWriter().close(); &#125; &#125; 效果： 这时候直接使用沙箱版扫码支付就可以了从此不用担心的问题，随心所欲，不用家里有矿也有花不完的钱！！！]]></content>
      <tags>
        <tag>JavaWeb</tag>
        <tag>支付接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决不蒜子统计不可用问题]]></title>
    <url>%2Farchives%2F8f7c4fe6.html</url>
    <content type="text"><![CDATA[10月份开始引用不蒜子作为静态网站的博客的统计功能都无法显示了，刚开始我还以为是网络原因，无法响应导致的。但是上不蒜子的官网看了一下，才知道，原来不蒜子的域名更改了，导致script引用不了，从而无法进行统计。 [00:00.780]멈춘 시간 속 [00:03.220]잠든 너를 찾아가 [00:05.430]아무리 막아도 [00:07.440]결국 너의 곁인 걸 [00:09.970]길고 긴 여행을 끝내 [00:12.890]이젠 돌아가 [00:15.060]너라는 집으로 [00:17.310]지금 다시 [00:18.310]way back home [00:40.140]아무리 힘껏 닫아도 [00:42.510]다시 열린 서랍 같아 [00:44.840]하늘로 높이 날린 넌 [00:47.340]자꾸 내게 되돌아와 [00:49.700]힘들게 삼킨 이별도 [00:52.210]다 그대로인 걸 [00:54.260]oh oh oh [00:57.710]수없이 떠난 길 위에서 [01:00.550]난 너를 발견하고 [01:02.810]비우려 했던 맘은 또 [01:05.350]이렇게 너로 차올라 [01:07.850]발걸음의 끝에 [01:10.250]늘 니가 부딪혀 [01:12.550]그만 [01:14.940]그만 [01:17.560]멈춘 시간 속 [01:19.500]잠든 너를 찾아가 [01:22.230]아무리 막아도 [01:24.210]결국 너의 곁인 걸 [01:26.660]길고 긴 여행을 끝내 [01:29.690]이젠 돌아가 [01:31.870]너라는 집으로 [01:33.740]지금 다시 [01:34.900]way back home [01:55.250]조용히 잠든 방을 열어 [01:58.070]기억을 꺼내 들어 [02:00.410]부서진 시간 위에서 [02:02.680]선명히 너는 떠올라 [02:05.300]길 잃은 맘 속에 [02:07.710]널 가둔 채 살아 [02:10.100]그만 [02:12.530]그만 [02:15.200]멈춘 시간 속 [02:17.260]잠든 너를 찾아가 [02:19.770]아무리 막아도 [02:21.820]결국 너의 곁인 걸 [02:24.220]길고 긴 여행을 끝내 [02:27.220]이젠 돌아가 [02:29.330]너라는 집으로 [02:31.260]지금 다시 [02:32.480]way back home [02:34.750]세상을 뒤집어 [02:37.090]찾으려 해 [02:39.660]오직 너로 완결된 [02:41.940]이야기를 [02:45.010]모든 걸 잃어도 [02:49.580]난 너 하나면 돼 [03:02.720]빛이 다 꺼진 여기 [03:05.610]나를 안아줘 [03:13.150]눈을 감으면 [03:14.520]소리 없이 밀려와 [03:17.440]이 마음 그 위로 [03:19.390]넌 또 한 겹 쌓여가 [03:21.880]내겐 그 누구도 아닌 [03:24.740]니가 필요해 [03:27.070]돌아와 내 곁에 [03:29.340]그날까지 [03:30.380]I’m not done var ap = new APlayer({ element: document.getElementById("aplayer-DZcSxZWm"), narrow: false, autoplay: false, showlrc: 2, music: { title: "Way Back Home", author: "SHAUN", url: "//music.163.com/song/media/outer/url?id=863046037.mp3", pic: "//p1.music.126.net/MAkLvm2p9LE0mWLEr2NkMA==/109951163378634466.jpg?param=130y130", } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 下面是不蒜子的官方公告： 123456大家好，因七牛强制过期原有的『dn-lbstatics.qbox.me』域名（预计2018年10月初），与客服沟通数次无果，即使我提出为此付费也不行，只能更换域名到『busuanzi.ibruce.info』！因我是最早的一批七牛用户，为七牛至少带来了数百个邀请用户，很痛心，很无奈！各位继续使用不蒜子提供的服务，只需把原有的：&lt;script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt;域名改一下即可：&lt;script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt;只需要修改该js域名，其他均未改变。若有疑问，可以加入不蒜子交流QQ群：`419260983`，对您带来的不便，非常抱歉！！！还是那句话，不蒜子不会中断服务！！！！ 因此，想要继续使用不蒜子功能，只能修改NexT的配置了。 只需要将 ..\next\layout_third-party\analytics下的 busuanzi-counter.swig修改成不蒜子新的域名就可以正常显示了 例子： 1&lt;script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 将上面的代码替换成下面的代码即可： 1&lt;script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解除Dr.com的WiFi限制，开启热点]]></title>
    <url>%2Farchives%2Fadea6c28.html</url>
    <content type="text"><![CDATA[前言现在国内大部分高校都使用了宽带上网认证，有一些比较严格的客户端还会禁止开启WiFi共享。作为8102年的新人类，怎么能没有WiFi这一必需品呢？ 下面就来记录一下，在Dr.com客户端下破解共享限制的功能，实现WiFi共享上网冲浪。 解除WiFi限制在校园网中使用Dr.com连接的用户，如果检测到存在共享行为，就会迫使账户下线；所以为了能够解除WiFi限制，需要安装一个插件才可以使Dr.com客户端跳过检测WiFi共享的行为。 破解WiFi限制，点击下载插件 下载工具包之后，直接运行该软件，然后点击 破解Dr.com 即可，破解WiFi限制； 破解后校园网可以随意使用猎豹WiFi、360随身wifi共享网络，也能使用win10自带的热点功能建立热点。 如果是使用猎豹WiFi开启热点的话，手机挂VP(屁)N(恩)是翻不过去的。但是如果是Win10自带的工具，就可以科学上网。具体原因尚未找到 :( 使用命令开启WiFi 首先开启wifi需要管理员权限，所以在开始菜单搜索cmd或组合键win+r，右键以管理员身份运行。 输入netsh wlan set hostednetwork mode=allow 回车，把承载网络模式设置为允许 输入netsh wlan set hostednetwork ssid=无线网络的名称 key=您想要设置的密码 名称跟密码直接写就行，不用双引号什么的 ssid跟key用空格隔开 密码必须8位以上 输入netsh wlan start hostednetwork回车开启wifi 这时候虽然开启了wifi，但是还是这时候还是无互联网连接的，所以需要有网络的网络连接共享给它 进入控制面板\所有控制面板项\网络和共享中心选择已经连接上互联网的网络，设置允许其他用户共享网络；具体步骤参考下图 其他命令1234#删除共享网络netsh wlan set hostednetwork mode=disallow# 查看共享网络信息---蹭网信息netsh wlan show hostednetwork 如果想要更加方便一点，可以把这些命令做成bat文件 新建文本保存以上代码，然后把文本文件以改成bat后缀。双击运行就可以了]]></content>
      <tags>
        <tag>Dr.com</tag>
        <tag>校园网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[To be happier]]></title>
    <url>%2Farchives%2F35de7772.html</url>
    <content type="text"><![CDATA[(function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"video":{"url":"//gss3.baidu.com/6LZ0ej3k1Qd3ote6lo7D0j9wehsv/tieba-smallvideo-transcode/10839346_255b960c73ccfcbd2602d6188e9089d4_0.mp4"},"danmaku":{"id":"b498e2397a2e40fafa94eea866e123861a02e6c779b3b84ef5dde735e9252cfa68a6b54b78b523962cd772b8989df2","api":"//api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})() 今天在逛油管的时候，发现了这个有点小感人、心酸的短视频。第一次看的时候没有看懂，后来多看几遍之后，才慢慢有一种代入感。 谁的生活都有低谷和困难的时候，如果一直沉浸在悲伤中，无法走出，那就只能停留在低谷，无法看到更高处的美丽风景；在低谷时，更要保持乐观的心态，无论多艰难都要咬着牙坚持下去，胜利就在下一刻。加油！！！ 视频的发布者留言：当@bastilledan第一次让我更快乐的时候，我被带回去了。你能在歌词中感受到的情感是疯狂的。当我越来越多地听这首歌时，它让我想起了我生命中经历过的许多事情。最近我失去了我最好的朋友，我的伴侣，我的终身朋友。我想要这个视频拥抱我们失去或牺牲的痛苦以及随之而来的伤痛。我希望这首歌和这段视频能让你在任何情况下都能得到解脱，让我们都更快乐。 ps: 由于优酷的外链不能禁止自动播放，为了用户体验，只能换百度贴吧的链接，可能清晰度有点低，能看就好~~]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NexT填坑-样式无法加载]]></title>
    <url>%2Farchives%2Fddf26aa7.html</url>
    <content type="text"><![CDATA[The sun always comes after the rain自从启用了 https之后，有一些资源文件加载导致网站提示不安全，从而绿锁丢失。百度了一番后，用meta升级HTTP请求的方法，在.\themes\next\layout\_partials\head.swing中添加了1&lt;meta http-equiv=”Content-Security-Policy” content=”upgrade-insecure-requests”/&gt; 部署上去之后，本来以为可以完美解决。但是到想添加插件更新博文的时候，本地部署测试，发现样式无法加载 搞了一个晚上，最后才发现就是添加了上面的那个代码才导致的这个问题。 把上面的代码去掉之后就可以，解决样式加载不了的问题 如果想引用http资源可以使用相对协议的方法,帮助你实现当网站引入的都是 http 资源，网站域名更换为 https 后的无缝切换。 具体使用方法为： 1&lt;img src="//domain.com/img/logo.png"&gt; 简而言之，就是将URL的协议（http、https）去掉，只保留//及后面的内容。这样，在使用https的网站中，浏览器会通过https请求URL，否则就通过http发送请求。 附注：如果是浏览本地文件，浏览器通过file://协议发送请求，导致请求失败，因此本地测试最好是搭建一个本地服务器。]]></content>
      <categories>
        <category>NexT</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NexT主题添加音乐]]></title>
    <url>%2Farchives%2Fbda67445.html</url>
    <content type="text"><![CDATA[NexT主题添加音乐在Hexo博客中添加音乐，有三种方式可以插入音乐 因为启用了 https，引用网易云的音乐会提示网站不安全，现在这个问题还没有解决，所以就不预览效果了 一、使用html标签写法如下： 1&lt;audio src="https://什么什么什么.mp3" style="max-height :100%; max-width: 100%; display: block; margin-left: auto; margin-right: auto;" controls="controls" loop="loop" preload="meta"&gt;Your browser does not support the audio tag.&lt;/audio&gt; 二、使用网易云外链网易云音乐的外链很好用，不仅有可以单曲，还能有歌单，有兴趣的自己去网易云音乐找首歌尝试。但是有一些音乐因为版权原因放不了，还有就是不完全支持 https，导致小绿锁不见了。 网易云歌曲外链接获取方法首先 找到你要下载的歌曲 用网页版打开 复制链接中的歌曲ID 如：SHAUN - Way Back Home1https://music.163.com/#/song?id=863046037 ID就是863046037然后将ID替换到下面的链接中1http://music.163.com/song/media/outer/url?id= .mp3 如：1http://music.163.com/song/media/outer/url?id=863046037.mp3 三、安装插件安装插件可以完美的解决上面的问题，并且用插件，有显示歌词功能，也美观，建议使用这种方法。 安装插件首先在站点文件夹根目录安装插件： 所在目录：~/blog/1npm install hexo-tag-aplayer --save 使用方法一在文章中的写法：1&#123;% aplayer title author url [picture_url, narrow, autoplay, width:xxx, lrc:xxx] %&#125; 标签参数 title : 曲目标题 author: 曲目作者 url: 音乐文件 URL 地址 picture_url: (可选) 音乐对应的图片地址 narrow: （可选）播放器袖珍风格 autoplay: (可选) 自动播放，移动端浏览器暂时不支持此功能 width:xxx:(可选) 播放器宽度(默认: 100%) lrc:xxx: (可选）歌词文件 URL 地址 实例1&#123;% aplayer "歌曲名" "歌手名" "https://什么什么什么.mp3" "https://封面图.jpg" "lrc:https://歌词.lrc" %&#125; 方法二除了使用标签 lrc 选项来设定歌词，你也可以直接使用 aplayerlrc 标签来直接插入歌词文本在博客中：123&#123;% aplayerlrc "title" "author" "url" "封面(选填)" "autoplay" %&#125;[00:00.00]lrc here&#123;% endaplayerlrc %&#125; 更多详细使用方法参考文档：hexo-tag-aplayer 获取歌词歌词的获取，可以直接找到各层次文件，或者可以直接在网易云上通过以下方法获取1http://music.163.com/api/song/media?id=863046037 其中id为网易云歌曲的id，打开链接之后，可以把”lyric”字段的值复制下来，再删除\n就可以直接放到aplayerlrc标签中了，这样就可以有歌词出现]]></content>
      <categories>
        <category>NexT</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[leetcode02]9-回文数]]></title>
    <url>%2Farchives%2F4644f3b9.html</url>
    <content type="text"><![CDATA[题目第9题：回文数 判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序(从右向左)读都是一样的整数。示例 1: 输入: 121输出: true 示例 2: 输入: -121输出: false 解释: 从左向右读, 为 -121 。 从右向左读, 为 121- 。因此它不是一个回文数。 示例 3: 输入: 10输出: false 解释: 从右向左读, 为 01 。因此它不是一个回文数。 进阶:你能不将整数转为字符串来解决这个问题吗？ 思路&amp;实现思路：先取出各个数字变成list，再翻转；然后判断新的list与原来的是否一致123456789101112131415class Solution: def isPalindrome(self, x): """ 先取出各个数字组成list，再翻转 :type x: int :rtype: bool """ li = [] if x &lt; 0: return False else: while x != 0: li.append(x % 10) x = int(x / 10) return list(reversed(li)) == li 官方解释首先，我们应该处理一些临界情况。所有负数都不可能是回文，例如：-123 不是回文，因为 - 不等于 3。所以我们可以对所有负数返回 false。 现在，让我们来考虑如何反转后半部分的数字。 对于数字 1221，如果执行 1221 % 10，我们将得到最后一位数字 1，要得到倒数第二位数字，我们可以先通过除以 10 把最后一位数字从 1221 中移除，1221 / 10 = 122，再求出上一步结果除以10的余数，122 % 10 = 2，就可以得到倒数第二位数字。如果我们把最后一位数字乘以10，再加上倒数第二位数字，1 * 10 + 2 = 12，就得到了我们想要的反转后的数字。 如果继续这个过程，我们将得到更多位数的反转数字。 现在的问题是，我们如何知道反转数字的位数已经达到原始数字位数的一半？ 我们将原始数字除以 10，然后给反转后的数字乘上 10，所以，当原始数字小于反转后的数字时，就意味着我们已经处理了一半位数的数字。 复杂度分析时间复杂度：O(\log_{10}(n))O(log 10 (n))， 对于每次迭代，我们会将输入除以10，因此时间复杂度为 O(\log_{10}(n))O(log10(n))。空间复杂度：O(1)O(1)。 实现根据官方的解释，我试着实现了一下：1234567891011121314def gaunfang(self, x): ''' 用时：400ms :param x: :return: ''' if x &lt; 0: return False temp_x = x y = 0 while temp_x != 0: y = y * 10 + temp_x % 10 temp_x = int(temp_x / 10) return y == x 其他方法1234567891011def reversed_str(self, x): ''' 翻转字符串 用时：446ms :param x: :return: ''' if str(x) == str(x)[::-1]: return True else: return False 这个方法主要是采用了字符串截取的方法，从最后逐位翻转，在判断。 题目难度难度：简单 总结这道题的算法难度不大，实现起来简单，但是没有想到的是，python的实现方法会是如此的简洁。看来，还是的多接触一下优秀的代码，增长一下自己的见识才行！！！]]></content>
      <categories>
        <category>python</category>
        <category>算法</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NexT建站历程]]></title>
    <url>%2Farchives%2F55e73569.html</url>
    <content type="text"><![CDATA[最近一段时间一直在倒腾着自己的个人博客。本来直接部署到Github上就算了，没想着弄域名或者搜索引擎优化啥的。但是前几天，忽而兴起，跑到阿里云上搜了一下域名，看到一个喜欢的域名还不错，并且价格也还算亲民。一时冲动就下了单…. 冲动是魔鬼，要不是买了个域名，或许就不会有这么多的烦心事。真的是“ No zuo , No die ”。折腾了好几天，终于弄好了https和引擎收录。加上了小绿锁，感觉就是不一样；虽然现在在搜索引擎上的权重还没上去，但是相信以后会好起来的。 说到搜索引擎，不得不吐槽一下国内和国外的效率。百度都提交一个星期了，才搜得到。人家谷歌第二天就把大部分文章给索引到了。还有一点最烦人的就是，Github 是禁止百度爬虫的 :( 搞得我还得双线部署。幸好，最后还是依靠国内的 Coding ，才让百度爬虫爬取到我的站点，并收录了进去。 都说前人栽树，后人乘凉。感谢各位博主的博客，才让我少踩了很多坑。以后就可以安心的写博客，不用管这些烦心事了。 为了记录一下自己的艰辛填坑路，也为了让更多想搭建博客的人能够 少踩一点坑。下面将献上一些对我有帮助的博客链接，希望也能帮到正在折腾的你！！ GitHub Pages 绑定个人域名，免 Cloudflare 支持 https 打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化 Hexo提交百度和Google收录站点 hexo的next主题个性化教程:打造炫酷网站 号外号外！解决github+hexo+yilia评论插件的问题！！！ 总的来说这几篇博文，已经涵盖了 NexT 主题的所有配置和优化过程。至于其他主题可以自行找教程，不过还是建议选用多人用的主题，避免钻牛角尖，既浪费时间，又难受，何必呢是吧？ 最后，当然是宣传一波我的站点啦。。。欢迎访问分享 ，反馈意见哈！！ https://www.donlex.cn]]></content>
      <categories>
        <category>随笔</category>
        <category>NexT</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>NexT</tag>
        <tag>hexo</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode打卡-1]]></title>
    <url>%2Farchives%2Fb6c00922.html</url>
    <content type="text"><![CDATA[前言从今天开始，将会开启我的 LeetCode 打卡之路。为了能让打卡坚持下去，就给自己设定了用博客来记录打卡过程的目标。算是留点记录吧！希望将来会感谢今天努力刷 LeetCode 的自己 : )~ 题目给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 思路采用逆向思维，用目标的值减去数组中的一个数，看结果是否还在数组里面 实现代码实现使用的是 python 语言 1234567891011121314class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ n = len(nums) for x in range(n): b = target-nums[x] if b in nums: y = nums.index(b) if y!=x: return x,y 题目难度难度：简单]]></content>
      <categories>
        <category>python</category>
        <category>算法</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python or Java？Boss直聘告诉你该如何选择]]></title>
    <url>%2Farchives%2Fe7a46e9a.html</url>
    <content type="text"><![CDATA[前言“人生苦短，我用 Python”，Python 的经典 slogan 讲究争分夺秒，并且在 9月的TIOBE榜中拿下第 3 名宝座。 今天就试着在Boss直聘网站上爬取python和java的招聘信息，比较一下两个方向的发展钱景，为本科生的就业方向给一个小小的建议 爬取在招聘网站上直接以”本科生”和”java”或”python”作为筛选条件，以广州为例爬取招聘的大体信息，具体代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from bs4 import BeautifulSoupimport requestsimport pymongoclient = pymongo.MongoClient('localhost', 27017)zhipin = client['zhipin']zhipin_java = zhipin['zhipin_java']zhipin_python = zhipin['zhipin_python']headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',&#125;total_page = 11def get_info(param, data_table): ''' 根据招聘方向(java或python..)爬取信息存进数据库 :param param: 招聘方向 :param data_table: 数据库表明 :return: ''' for i in range(1, total_page): url = 'https://www.zhipin.com/c101280100/d_203-h_101280100/?query=&#123;0&#125;&amp;page=&#123;1&#125;'.format( param, i) web_data = requests.get(url, headers=headers) soup = BeautifulSoup(web_data.content, 'lxml') for item in soup.select('#main &gt; div &gt; div.job-list &gt; ul &gt; li'): # 招聘要求 job_title = item.select('.job-title')[0].text # 岗位 salary = item.select('.red')[0].text # 薪资 person_info = item.select('.info-primary p')[0].text # 应聘要求 # 获取公司信息 company = item.select('.info-company h3 a')[0].text # 公司 company_info = item.select('.info-company p')[0].text # 公司信息 data = &#123; 'job_title': job_title, 'salary': salary, 'person_info': person_info, 'company': company, 'company_info': company_info, &#125; # 插入数据库 data_table.insert(data) print(data) print('*' * 100) print('\n' * 5)if __name__ == '__main__': param_list = ['java', 'python'] table_list = [zhipin_java, zhipin_python] for param, table in zip(param_list, table_list): get_info(param, table) 爬取的信息全部存在mongodb中。便于后面的分析处理 数据清洗在数据处理这里定义了几个方法，用来处理相应的内容 1.初始化变量12345678import pymongoclient = pymongo.MongoClient('localhost', 27017)zhipin = client['zhipin']zhipin_java = zhipin['zhipin_java']zhipin_python = zhipin['zhipin_python']from collections import Counterfrom pyecharts import Bar,Line,Pie 2.获取地区分布情况123456789101112131415import redef get_zone(): ''' 获取地区''' zone_list = [] real_list = [] for item in zhipin_java.find(): text = item['person_info'][3:6] zone_list.append(text) for i in zone_list: j = re.sub(r' \d-','',i) real_list.append(j) while '' in real_list: real_list.remove('') return real_listzone = dict(Counter(get_zone())) 3.整理招聘数据123456789def del_key_1(): '''删除招聘次数为1的岗位''' li = [] for key in job_dict.keys(): if job_dict[key] == 1: li.append(key) for i in li: del job_dict[i] print(job_dict) 4.整理薪水数据1234567891011def get_salary(): '''获取招聘的工资''' min_list = [] #起步工资 max_list = [] #最高工资 job_title = [] #岗位 for item in zhipin_java.find(): job_title.append(item['job_title']) salary = item['salary'] min_list.append(int(salary.split('-')[0][:-1])) max_list.append(int(salary.split('-')[1][:-1])) return min_list,max_list,job_title 数据可视化地区分布通过整理地区分布数据,利用pyecharts作图 1234bar = Bar("java和python岗位地区分布")bar.add("java", list(key for key in zone.keys()), list(value for value in zone.values()),mark_line=['min', 'max'], is_toolbox_show = True,is_more_utils=True)bar.add("python", list(key for key in py_zone.keys()), list(value for value in py_zone.values()),mark_line=['min', 'max'], is_toolbox_show = True,is_more_utils=True)bar 越靠近城市中心的地区，招聘的岗位就越多，成功应聘的机会较高；番禺和天河区相差较大，其中天河区招python比java将近多8倍；番禺区java比python更加热门，受公司青睐；其他区相差不大 招聘最多的岗位python岗位： 占比前五位分别是： python工程师 数据分析师 运维工程师 大数据开发工程师 游戏AI算法工程师 java岗位对比 高级的工程师招聘的人数较少，大部分都是在招聘初中级工程师，难道这就是传说中的“一个诸葛亮胜过三个臭皮匠 (:” 公司对比python招聘公司 java招聘公司 最关心的钱途问题最高薪水 看来python不是吹的，最高薪水也大多数都比java的高;java最高薪水平均19.24K，最低3K，最高50k；python最高薪水平均21.16K,最低3k，最高60k 最低薪水 python起步薪水大多数都比java的高;java平均起步薪水11.42K，python平均起步薪水12.08K 两个岗位词云 源码：https://github.com/stormdony/python_demo CSDN博客：https://blog.csdn.net/stormdony/article/details/82586735 1ps: 原创文章，转载请与作者联系]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo下yilia主题添加字数统计和阅读时长功能]]></title>
    <url>%2Farchives%2Fe9dc5279.html</url>
    <content type="text"><![CDATA[1.安装 hexo-wordcount在博客目录下打开Git Bash Here 输入命令 npm i –save hexo-wordcount 2.文件配置在theme\yilia\layout\_partial\post下创建word.ejs文件： 123456789101112131415161718&lt;div style="margin-top:10px;"&gt; &lt;span class="post-time"&gt; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-keyboard-o"&gt;&lt;/i&gt; &lt;span class="post-meta-item-text"&gt; 字数统计: &lt;/span&gt; &lt;span class="post-count"&gt;&lt;%= wordcount(post.content) %&gt;字&lt;/span&gt; &lt;/span&gt; &lt;/span&gt; &lt;span class="post-time"&gt; &amp;nbsp; | &amp;nbsp; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-hourglass-half"&gt;&lt;/i&gt; &lt;span class="post-meta-item-text"&gt; 阅读时长: &lt;/span&gt; &lt;span class="post-count"&gt;&lt;%= min2read(post.content) %&gt;分&lt;/span&gt; &lt;/span&gt; &lt;/span&gt;&lt;/div&gt; 然后在 themes/yilia/layout/_partial/article.ejs中添加123456789101112131415&lt;div class="article-inner"&gt; &lt;% if (post.link || post.title)&#123; %&gt; &lt;header class="article-header"&gt; &lt;%- partial('post/title', &#123;class_name: 'article-title'&#125;) %&gt; &lt;% if (!post.noDate)&#123; %&gt; &lt;%- partial('post/date', &#123;class_name: 'archive-article-date', date_format: null&#125;) %&gt; &lt;!-- 需要添加的位置 --&gt; &lt;!-- 开始添加字数统计--&gt; &lt;% if(theme.word_count &amp;&amp; !post.no_word_count)&#123;%&gt; &lt;%- partial('post/word') %&gt; &lt;% &#125; %&gt; &lt;!-- 添加完成 --&gt; &lt;% &#125; %&gt; &lt;/header&gt; 3. 开启功能在站点的_config.yml中添加下面代码123# 是否开启字数统计#不需要使用，直接设置值为false，或注释掉word_count: True ps：原创文章，转载请注明出处]]></content>
      <categories>
        <category>yilia</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>yilia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加和取消live2d看板动画]]></title>
    <url>%2Farchives%2F14d1161.html</url>
    <content type="text"><![CDATA[添加看板娘在博客目录下安装依赖 npm install –save hexo-helper-live2d 在主题下的_config.yml的配置信息Hexo的 _config.yml 文件添加配置. 示例:12345678910111213141516live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: use: live2d-widget-model-wanko display: position: right width: 150 height: 300 mobile: show: false 使用其他的模型，需要先安装模型,在修改配置信息中的use 查看模型：https://github.com/xiazeyu/live2d-widget-models 截图预览：https://huaji8.top/post/live2d-plugin-2.0/ 安装模型 npm install 模型的包名 具体可以查看官方文档：https://github.com/EYHN/hexo-helper-live2d/blob/master/README.zh-CN.md 取消看板娘直接运行下面的命令 npm uninstall hexo-helper-live2d 去掉站点_config.yml下的配置信息即可 ps：原创文章，转载请注明出处]]></content>
      <categories>
        <category>yilia</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>yilia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单选题：安全和便捷，你选哪一个？]]></title>
    <url>%2Farchives%2F4ec71def.html</url>
    <content type="text"><![CDATA[前段时间的滴滴杀人案可谓是闹得满城风雨，但还是禁不住时间这把杀猪刀，人们的眼球又被“东哥”的案子给吸引过去了。虽然吃瓜看戏不错，但是也要关注一下关乎自身利益的问题。9月4日晚，滴滴出行宣布将于2018年9月4日启动安全大整治，2018年9月8日23点至9月15日凌晨5点期间在中国大陆地区暂停提供深夜23：00-5：00时间段的网约车。 这一措施一旦实行，将会在出行和安全方面带来怎样的变化？ 首先很多加班族和夜猫子可能会心慌的一批。深夜刚加完班，公交、地铁都停了，只能在路边使劲摇胳膊喊“师傅”，本来上班的压力就山大了，现在可能还要体验一把抢车风波；而蹦迪的、买醉的还有撸串的出行难度也会增加，最后可能会放弃，选择宅起来。 1.出行成本相对于出租,虽然滴滴价格并没有低太多,但还是便宜一些。并且网约车的接单机制与出租车或者其他的出行方式的不一样。网约车可以自己设定时间，更加迁就自己，不必浪费时间在等车上；而其他的方式则有一定的时间随机性。在时间成本上，网约车的消耗成本就低了很多。 另外在网约车的市场上，滴滴占了63%的市场份额。而停止深夜服务则会更加加剧出行选择的稀缺性。“物以稀为贵”的道理大家都明白。这样可能不仅是时间和金钱成本上的的增加了。 2.安全 | 便捷滴滴的几次用户遇害案件，使得乘客的警惕意识加强，同时也让社会对网约车的有了防备。很多人虽然也对先前发生的事情义愤填膺，但是经受不了站街摇手苦等，于是会想，滴滴用户量那么大，遭遇不幸的事情总不会落到我头上吧，并且滴滴还上线了“一键报警”功能… 虽然想象很美好，但是在整改的成效没有体现出来之前，安全和便捷不能同时满足。而滴滴的做法意思就很明显了。我们也只能牺牲便捷来换取安全保障的提高了。 3.用户心理网约车跟私人订制类似，而出租和其他的出行方式是面向大众的。这一区别使得用户的体验感不一样，一个是消费者是上帝（真实是不是这样就另说了），另一个则体现不出个人的与众不同。 最后试问一句：如果没有安全方面的问题，你愿意在路边苦等出租，还是使用滴滴这样的定制服务？？ ps：纯属个人看法，有不成熟之处请谅解！！]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫利器-cURL转换]]></title>
    <url>%2Farchives%2Ffd7c0b75.html</url>
    <content type="text"><![CDATA[前言在爬虫的过程，经常需要为程序添加请求头，参数，cookie等信息，但是这些信息的添加都需要手动的去浏览器中找，然后一点一点的慢慢复制粘贴，这样效率就非常的低了。今天就分享一个网站，解决这些问题，让你脱离这些没有意义的劳动 网站介绍网址: https://curl.trillworks.com 从上图可以看到网站的教程，只要根据教程三步走，就可以快速的添加相应的请求信息 示范 将需要爬取的请求复制curl到网站中转换，然后复制到pycharm中就可以直接爬取到整个网站的源码了，接下来就可以直接在这个基础上开始逻辑工作了 生成的代码：123456789101112131415161718192021222324import requestscookies = &#123; '_octo': 'GH1.1.681056136.1509806877', '_gat': '1', 'logged_in': 'no', '_ga': 'GA1.2.70269906.1509806877', 'tz': 'Asia%2FShanghai', 'has_recent_activity': '1', '_gh_sess': 'cGpmdExmZUZpckZ0R1pSQlFxZlpsS2ZvT3NZbUU0YW1qTVloSzdFeWNxeWdNaGxsNzVveTJ3Vndrc2ZaN3ZoRDNYMm10TW9OdUdGVHhwbVRmMEU3ZWVwTUx4dUpZTUgrbHdKZkV0RnpzN3hodG12TGdLbHpSemVaQ0ZMM201MGdxMlkxdk5JNUZ6em1SWGp5ZEJUYTNQMjRFcCtqUDZaWVVFNXl3VDJRRUU4MFpqYkpvekY1VmZpY2t1R01ZcGRPQlZBUEJUOTJaWnNESjVnMnlkcncyWWhCVDl1OE5aVDhpR2Z4Z1NYVkFVNk5ReDRtTVphOXFXQWJNSVZYcnEyVktLTERLMHBTYjNwa2tUQUJaaWREQ0N4NzJYTG9sM1dpUktPaWFETFVpWGZlWFNvb2ZxazU1OUxMazVjZ3VNNTJteEdENzJPQlFKeDV3YXZCbmdHSGdGVmx5OVNjU2VaZXh3eEVwSlptczZXV3lQZXgrOGEyVGFwcUpPcFhIZTRWaDIwZExMRWhDRE8yMUdJT2xmS1grQ3I3bEYySGJvWFhNTFR3VmNpRnlLTT0tLXlRMmJZanl4Z0tUU0c0N1ZrRHpqbkE9PQ%3D%3D--1899440138004359a97b156d0ac8941135684ab5',&#125;headers = &#123; 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer': 'https://ghbtns.com/github-btn.html?user=NickCarneiro&amp;repo=curlconverter&amp;type=watch&amp;count=true&amp;size=large', 'Connection': 'keep-alive', 'Cache-Control': 'max-age=0',&#125;response = requests.get('https://github.com/NickCarneiro/curlconverter/', headers=headers, cookies=cookies) 可以看到生成的代码非常的规整，是不是很方便~~]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Farchives%2F4a17b156.html</url>
    <content type="text"><![CDATA[Welcome to DonLex’s blog. This blog has just been established, so here is few pages, it will be improved gradually.If you have any problems, you can send E-mail to donlex@qq.com]]></content>
  </entry>
</search>
