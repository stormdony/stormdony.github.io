{"meta":{"title":"Don Lex","subtitle":"身体和灵魂总要有一个在路上","description":null,"author":"Don Lex","url":"https://stormdony.github.io"},"pages":[{"title":"about","date":"2018-09-13T07:01:11.000Z","updated":"2018-09-13T07:01:11.583Z","comments":true,"path":"about/index.html","permalink":"https://stormdony.github.io/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-09-09T06:47:02.000Z","updated":"2018-09-09T06:47:14.738Z","comments":true,"path":"tags/index.html","permalink":"https://stormdony.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-09-09T06:45:44.000Z","updated":"2018-09-09T06:46:09.806Z","comments":true,"path":"categories/index.html","permalink":"https://stormdony.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Python or Java？Boss直聘告诉你该如何选择","slug":"Python-or-Java？Boss直聘告诉你该如何选择","date":"2018-09-10T03:13:33.000Z","updated":"2018-09-10T03:40:10.560Z","comments":true,"path":"2018/09/10/Python-or-Java？Boss直聘告诉你该如何选择/","link":"","permalink":"https://stormdony.github.io/2018/09/10/Python-or-Java？Boss直聘告诉你该如何选择/","excerpt":"前言“人生苦短，我用 Python”，Python 的经典 slogan 讲究争分夺秒，并且在 9月的TIOBE榜中拿下第 3 名宝座。","text":"前言“人生苦短，我用 Python”，Python 的经典 slogan 讲究争分夺秒，并且在 9月的TIOBE榜中拿下第 3 名宝座。 今天就试着在Boss直聘网站上爬取python和java的招聘信息，比较一下两个方向的发展钱景，为本科生的就业方向给一个小小的建议 爬取在招聘网站上直接以”本科生”和”java”或”python”作为筛选条件，以广州为例爬取招聘的大体信息，具体代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from bs4 import BeautifulSoupimport requestsimport pymongoclient = pymongo.MongoClient('localhost', 27017)zhipin = client['zhipin']zhipin_java = zhipin['zhipin_java']zhipin_python = zhipin['zhipin_python']headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',&#125;total_page = 11def get_info(param, data_table): ''' 根据招聘方向(java或python..)爬取信息存进数据库 :param param: 招聘方向 :param data_table: 数据库表明 :return: ''' for i in range(1, total_page): url = 'https://www.zhipin.com/c101280100/d_203-h_101280100/?query=&#123;0&#125;&amp;page=&#123;1&#125;'.format( param, i) web_data = requests.get(url, headers=headers) soup = BeautifulSoup(web_data.content, 'lxml') for item in soup.select('#main &gt; div &gt; div.job-list &gt; ul &gt; li'): # 招聘要求 job_title = item.select('.job-title')[0].text # 岗位 salary = item.select('.red')[0].text # 薪资 person_info = item.select('.info-primary p')[0].text # 应聘要求 # 获取公司信息 company = item.select('.info-company h3 a')[0].text # 公司 company_info = item.select('.info-company p')[0].text # 公司信息 data = &#123; 'job_title': job_title, 'salary': salary, 'person_info': person_info, 'company': company, 'company_info': company_info, &#125; # 插入数据库 data_table.insert(data) print(data) print('*' * 100) print('\\n' * 5)if __name__ == '__main__': param_list = ['java', 'python'] table_list = [zhipin_java, zhipin_python] for param, table in zip(param_list, table_list): get_info(param, table) 爬取的信息全部存在mongodb中。便于后面的分析处理 数据清洗在数据处理这里定义了几个方法，用来处理相应的内容 1.初始化变量12345678import pymongoclient = pymongo.MongoClient('localhost', 27017)zhipin = client['zhipin']zhipin_java = zhipin['zhipin_java']zhipin_python = zhipin['zhipin_python']from collections import Counterfrom pyecharts import Bar,Line,Pie 2.获取地区分布情况123456789101112131415import redef get_zone(): ''' 获取地区''' zone_list = [] real_list = [] for item in zhipin_java.find(): text = item['person_info'][3:6] zone_list.append(text) for i in zone_list: j = re.sub(r' \\d-','',i) real_list.append(j) while '' in real_list: real_list.remove('') return real_listzone = dict(Counter(get_zone())) 3.整理招聘数据123456789def del_key_1(): '''删除招聘次数为1的岗位''' li = [] for key in job_dict.keys(): if job_dict[key] == 1: li.append(key) for i in li: del job_dict[i] print(job_dict) 4.整理薪水数据1234567891011def get_salary(): '''获取招聘的工资''' min_list = [] #起步工资 max_list = [] #最高工资 job_title = [] #岗位 for item in zhipin_java.find(): job_title.append(item['job_title']) salary = item['salary'] min_list.append(int(salary.split('-')[0][:-1])) max_list.append(int(salary.split('-')[1][:-1])) return min_list,max_list,job_title 数据可视化地区分布通过整理地区分布数据,利用pyecharts作图 1234bar = Bar(\"java和python岗位地区分布\")bar.add(\"java\", list(key for key in zone.keys()), list(value for value in zone.values()),mark_line=['min', 'max'], is_toolbox_show = True,is_more_utils=True)bar.add(\"python\", list(key for key in py_zone.keys()), list(value for value in py_zone.values()),mark_line=['min', 'max'], is_toolbox_show = True,is_more_utils=True)bar 越靠近城市中心的地区，招聘的岗位就越多，成功应聘的机会较高；番禺和天河区相差较大，其中天河区招python比java将近多8倍；番禺区java比python更加热门，受公司青睐；其他区相差不大 招聘最多的岗位python岗位： 占比前五位分别是： python工程师 数据分析师 运维工程师 大数据开发工程师 游戏AI算法工程师 java岗位对比 高级的工程师招聘的人数较少，大部分都是在招聘初中级工程师，难道这就是传说中的“一个诸葛亮胜过三个臭皮匠 (:” 公司对比python招聘公司 java招聘公司 最关心的钱途问题最高薪水 看来python不是吹的，最高薪水也大多数都比java的高;java最高薪水平均19.24K，最低3K，最高50k；python最高薪水平均21.16K,最低3k，最高60k 最低薪水 python起步薪水大多数都比java的高;java平均起步薪水11.42K，python平均起步薪水12.08K 两个岗位词云 源码：https://github.com/stormdony/python_demo CSDN博客：https://blog.csdn.net/stormdony/article/details/82586735 1ps: 原创文章，转载请与作者联系","categories":[{"name":"python","slug":"python","permalink":"https://stormdony.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://stormdony.github.io/tags/python/"}]},{"title":"hexo下yilia主题添加字数统计和阅读时长功能","slug":"hexo下yilia主题添加字数统计和阅读时长功能","date":"2018-09-09T07:56:34.000Z","updated":"2018-09-09T08:12:05.506Z","comments":true,"path":"2018/09/09/hexo下yilia主题添加字数统计和阅读时长功能/","link":"","permalink":"https://stormdony.github.io/2018/09/09/hexo下yilia主题添加字数统计和阅读时长功能/","excerpt":"1.安装 hexo-wordcount在博客目录下打开Git Bash Here 输入命令 npm i –save hexo-wordcount","text":"1.安装 hexo-wordcount在博客目录下打开Git Bash Here 输入命令 npm i –save hexo-wordcount 2.文件配置在theme\\yilia\\layout\\_partial\\post下创建word.ejs文件： 123456789101112131415161718&lt;div style=\"margin-top:10px;\"&gt; &lt;span class=\"post-time\"&gt; &lt;span class=\"post-meta-item-icon\"&gt; &lt;i class=\"fa fa-keyboard-o\"&gt;&lt;/i&gt; &lt;span class=\"post-meta-item-text\"&gt; 字数统计: &lt;/span&gt; &lt;span class=\"post-count\"&gt;&lt;%= wordcount(post.content) %&gt;字&lt;/span&gt; &lt;/span&gt; &lt;/span&gt; &lt;span class=\"post-time\"&gt; &amp;nbsp; | &amp;nbsp; &lt;span class=\"post-meta-item-icon\"&gt; &lt;i class=\"fa fa-hourglass-half\"&gt;&lt;/i&gt; &lt;span class=\"post-meta-item-text\"&gt; 阅读时长: &lt;/span&gt; &lt;span class=\"post-count\"&gt;&lt;%= min2read(post.content) %&gt;分&lt;/span&gt; &lt;/span&gt; &lt;/span&gt;&lt;/div&gt; 然后在 themes/yilia/layout/_partial/article.ejs中添加123456789101112131415&lt;div class=\"article-inner\"&gt; &lt;% if (post.link || post.title)&#123; %&gt; &lt;header class=\"article-header\"&gt; &lt;%- partial('post/title', &#123;class_name: 'article-title'&#125;) %&gt; &lt;% if (!post.noDate)&#123; %&gt; &lt;%- partial('post/date', &#123;class_name: 'archive-article-date', date_format: null&#125;) %&gt; &lt;!-- 需要添加的位置 --&gt; &lt;!-- 开始添加字数统计--&gt; &lt;% if(theme.word_count &amp;&amp; !post.no_word_count)&#123;%&gt; &lt;%- partial('post/word') %&gt; &lt;% &#125; %&gt; &lt;!-- 添加完成 --&gt; &lt;% &#125; %&gt; &lt;/header&gt; 3. 开启功能在站点的_config.yml中添加下面代码123# 是否开启字数统计#不需要使用，直接设置值为false，或注释掉word_count: True ps：原创文章，转载请注明出处","categories":[{"name":"yilia","slug":"yilia","permalink":"https://stormdony.github.io/categories/yilia/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://stormdony.github.io/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"https://stormdony.github.io/tags/yilia/"}]},{"title":"hexo添加和取消live2d看板动画","slug":"hexo添加和取消live2d看板动画","date":"2018-09-09T06:31:09.000Z","updated":"2018-09-09T08:06:32.178Z","comments":true,"path":"2018/09/09/hexo添加和取消live2d看板动画/","link":"","permalink":"https://stormdony.github.io/2018/09/09/hexo添加和取消live2d看板动画/","excerpt":"添加看板娘在博客目录下安装依赖 npm install –save hexo-helper-live2d","text":"添加看板娘在博客目录下安装依赖 npm install –save hexo-helper-live2d 在主题下的_config.yml的配置信息Hexo的 _config.yml 文件添加配置. 示例:12345678910111213141516live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: use: live2d-widget-model-wanko display: position: right width: 150 height: 300 mobile: show: false 使用其他的模型，需要先安装模型,在修改配置信息中的use 查看模型：https://github.com/xiazeyu/live2d-widget-models 截图预览：https://huaji8.top/post/live2d-plugin-2.0/ 安装模型 npm install 模型的包名 具体可以查看官方文档：https://github.com/EYHN/hexo-helper-live2d/blob/master/README.zh-CN.md 取消看板娘直接运行下面的命令 npm uninstall hexo-helper-live2d 去掉站点_config.yml下的配置信息即可 ps：原创文章，转载请注明出处","categories":[{"name":"yilia","slug":"yilia","permalink":"https://stormdony.github.io/categories/yilia/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://stormdony.github.io/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"https://stormdony.github.io/tags/yilia/"}]},{"title":"单选题：安全和便捷，你选哪一个？","slug":"单选题：安全和便捷，你选哪一个？","date":"2018-09-07T11:07:04.000Z","updated":"2018-09-09T01:42:04.616Z","comments":true,"path":"2018/09/07/单选题：安全和便捷，你选哪一个？/","link":"","permalink":"https://stormdony.github.io/2018/09/07/单选题：安全和便捷，你选哪一个？/","excerpt":"前段时间的滴滴杀人案可谓是闹得满城风雨，但还是禁不住时间这把杀猪刀，人们的眼球又被“东哥”的案子给吸引过去了。虽然吃瓜看戏不错，但是也要关注一下关乎自身利益的问题。","text":"前段时间的滴滴杀人案可谓是闹得满城风雨，但还是禁不住时间这把杀猪刀，人们的眼球又被“东哥”的案子给吸引过去了。虽然吃瓜看戏不错，但是也要关注一下关乎自身利益的问题。9月4日晚，滴滴出行宣布将于2018年9月4日启动安全大整治，2018年9月8日23点至9月15日凌晨5点期间在中国大陆地区暂停提供深夜23：00-5：00时间段的网约车。 这一措施一旦实行，将会在出行和安全方面带来怎样的变化？ 首先很多加班族和夜猫子可能会心慌的一批。深夜刚加完班，公交、地铁都停了，只能在路边使劲摇胳膊喊“师傅”，本来上班的压力就山大了，现在可能还要体验一把抢车风波；而蹦迪的、买醉的还有撸串的出行难度也会增加，最后可能会放弃，选择宅起来。 1.出行成本相对于出租,虽然滴滴价格并没有低太多,但还是便宜一些。并且网约车的接单机制与出租车或者其他的出行方式的不一样。网约车可以自己设定时间，更加迁就自己，不必浪费时间在等车上；而其他的方式则有一定的时间随机性。在时间成本上，网约车的消耗成本就低了很多。 另外在网约车的市场上，滴滴占了63%的市场份额。而停止深夜服务则会更加加剧出行选择的稀缺性。“物以稀为贵”的道理大家都明白。这样可能不仅是时间和金钱成本上的的增加了。 2.安全 | 便捷滴滴的几次用户遇害案件，使得乘客的警惕意识加强，同时也让社会对网约车的有了防备。很多人虽然也对先前发生的事情义愤填膺，但是经受不了站街摇手苦等，于是会想，滴滴用户量那么大，遭遇不幸的事情总不会落到我头上吧，并且滴滴还上线了“一键报警”功能… 虽然想象很美好，但是在整改的成效没有体现出来之前，安全和便捷不能同时满足。而滴滴的做法意思就很明显了。我们也只能牺牲便捷来换取安全保障的提高了。 3.用户心理网约车跟私人订制类似，而出租和其他的出行方式是面向大众的。这一区别使得用户的体验感不一样，一个是消费者是上帝（真实是不是这样就另说了），另一个则体现不出个人的与众不同。 最后试问一句：如果没有安全方面的问题，你愿意在路边苦等出租，还是使用滴滴这样的定制服务？？ ps：纯属个人看法，有不成熟之处请谅解！！","categories":[{"name":"随笔","slug":"随笔","permalink":"https://stormdony.github.io/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://stormdony.github.io/tags/随笔/"}]},{"title":"爬虫利器-cURL转换","slug":"爬虫利器-cURL转换","date":"2018-09-06T02:56:02.000Z","updated":"2018-09-08T15:40:14.836Z","comments":true,"path":"2018/09/06/爬虫利器-cURL转换/","link":"","permalink":"https://stormdony.github.io/2018/09/06/爬虫利器-cURL转换/","excerpt":"前言在爬虫的过程，经常需要为程序添加请求头，参数，cookie等信息，但是这些信息的添加都需要手动的去浏览器中找，然后一点一点的慢慢复制粘贴，这样效率就非常的低了。今天就分享一个网站，解决这些问题，让你脱离这些没有意义的劳动","text":"前言在爬虫的过程，经常需要为程序添加请求头，参数，cookie等信息，但是这些信息的添加都需要手动的去浏览器中找，然后一点一点的慢慢复制粘贴，这样效率就非常的低了。今天就分享一个网站，解决这些问题，让你脱离这些没有意义的劳动 网站介绍网址: https://curl.trillworks.com 从上图可以看到网站的教程，只要根据教程三步走，就可以快速的添加相应的请求信息 示范 将需要爬取的请求复制curl到网站中转换，然后复制到pycharm中就可以直接爬取到整个网站的源码了，接下来就可以直接在这个基础上开始逻辑工作了 生成的代码：123456789101112131415161718192021222324import requestscookies = &#123; '_octo': 'GH1.1.681056136.1509806877', '_gat': '1', 'logged_in': 'no', '_ga': 'GA1.2.70269906.1509806877', 'tz': 'Asia%2FShanghai', 'has_recent_activity': '1', '_gh_sess': 'cGpmdExmZUZpckZ0R1pSQlFxZlpsS2ZvT3NZbUU0YW1qTVloSzdFeWNxeWdNaGxsNzVveTJ3Vndrc2ZaN3ZoRDNYMm10TW9OdUdGVHhwbVRmMEU3ZWVwTUx4dUpZTUgrbHdKZkV0RnpzN3hodG12TGdLbHpSemVaQ0ZMM201MGdxMlkxdk5JNUZ6em1SWGp5ZEJUYTNQMjRFcCtqUDZaWVVFNXl3VDJRRUU4MFpqYkpvekY1VmZpY2t1R01ZcGRPQlZBUEJUOTJaWnNESjVnMnlkcncyWWhCVDl1OE5aVDhpR2Z4Z1NYVkFVNk5ReDRtTVphOXFXQWJNSVZYcnEyVktLTERLMHBTYjNwa2tUQUJaaWREQ0N4NzJYTG9sM1dpUktPaWFETFVpWGZlWFNvb2ZxazU1OUxMazVjZ3VNNTJteEdENzJPQlFKeDV3YXZCbmdHSGdGVmx5OVNjU2VaZXh3eEVwSlptczZXV3lQZXgrOGEyVGFwcUpPcFhIZTRWaDIwZExMRWhDRE8yMUdJT2xmS1grQ3I3bEYySGJvWFhNTFR3VmNpRnlLTT0tLXlRMmJZanl4Z0tUU0c0N1ZrRHpqbkE9PQ%3D%3D--1899440138004359a97b156d0ac8941135684ab5',&#125;headers = &#123; 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer': 'https://ghbtns.com/github-btn.html?user=NickCarneiro&amp;repo=curlconverter&amp;type=watch&amp;count=true&amp;size=large', 'Connection': 'keep-alive', 'Cache-Control': 'max-age=0',&#125;response = requests.get('https://github.com/NickCarneiro/curlconverter/', headers=headers, cookies=cookies) 可以看到生成的代码非常的规整，是不是很方便~~","categories":[{"name":"python","slug":"python","permalink":"https://stormdony.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://stormdony.github.io/tags/python/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-09-05T02:26:45.363Z","updated":"2018-09-08T15:40:09.894Z","comments":true,"path":"2018/09/05/hello-world/","link":"","permalink":"https://stormdony.github.io/2018/09/05/hello-world/","excerpt":"","text":"Welcome to the DonLex’s blog. This blog has just been established, so there is few pages, it will be improved gradually.If you get any problems, you can send an E-mail to stormdony@foxmail.com","categories":[],"tags":[]}]}